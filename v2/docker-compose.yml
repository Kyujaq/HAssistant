# HAssistant v2 - Streaming Speech Stack
# Port Allocation:
#   - 11434: ollama-chat (Hermes3 + Qwen3-4B)
#   - 10300: wyoming_openai STT (Wyoming facade)
#   - 10210: wyoming_openai TTS (proxy to Piper; avoids conflict with native piper)
#   - 10200: wyoming-piper (native fallback TTS for A/B testing)
#   - 8085:  wyoming_openai HTTP healthcheck/config API
#
# IMPORTANT: Verify GPU indices with `nvidia-smi --query-gpu=index,name --format=csv`
#            Update NVIDIA_VISIBLE_DEVICES if your GPU1 has a different index

name: hassistant_v2

services:
  # Ollama text service - hosts Hermes3 (fast) and Qwen3-4B (deeper text) on GTX 1070
  ollama-text:
    image: ollama/ollama:latest
    container_name: hassistant_v2_ollama_text
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0          # Pin to GPU0 (GTX 1070)
      - OLLAMA_KEEP_ALIVE=3h
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=1             # Avoid competing with STT bursts
    ports:
      - "11435:11434"                     # Expose on 11435 for clarity
    volumes:
      - ollama_text_data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 90s
    restart: on-failure:3
    networks:
      default:
        aliases: ["ollama-text", "ollama-chat"]  # Keep ollama-chat alias for backwards compatibility

  # Ollama VL service - hosts Qwen-VL-8B for opportunistic text escalation on GTX 1080 Ti
  ollama-vl:
    image: ollama/ollama:latest
    container_name: hassistant_v2_ollama_vl
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=1          # Pin to GPU1 (GTX 1080 Ti)
      - OLLAMA_KEEP_ALIVE=3h
      - OLLAMA_HOST=0.0.0.0:11434
    ports:
      - "11436:11434"
    volumes:
      - ollama_vl_data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 90s
    restart: on-failure:3
    networks:
      default:
        aliases: ["ollama-vl"]

  tailscale:
    image: tailscale/tailscale:latest
    container_name: hassistant_v2_tailscale
    hostname: ${TAILSCALE_HOSTNAME:-hassistant-v2}
    restart: unless-stopped
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    devices:
      - /dev/net/tun:/dev/net/tun
    volumes:
      - tailscale_state:/var/lib/tailscale
      - tailscale_run:/var/run/tailscale
    environment:
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_SOCKET=/var/run/tailscale/tailscaled.sock
    command:
      - tailscaled
      - --state=/var/lib/tailscale/tailscaled.state
      - --socket=/var/run/tailscale/tailscaled.sock
      - --tun=tsdev0
  
  mosquitto:
    image: eclipse-mosquitto:2
    container_name: hassistant_v2_mosquitto
    restart: unless-stopped
    ports:
      - "1883:1883"   # MQTT broker port
      - "9001:9001"   # WebSocket (optional)
    volumes:
      - ./mosquitto/config:/mosquitto/config
      - ./mosquitto/data:/mosquitto/data
      - ./mosquitto/log:/mosquitto/log
    networks:
      default:
        aliases: ["mosquitto"]

  whisper-stt:
    build:
      context: ./services/whisper_stt
    container_name: hassistant_v2_whisper_stt
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - USE_CUDA=1
      - WHISPER_MODEL=${WHISPER_MODEL:-base.en}
      - WHISPER_COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE:-int8_float16}
      - WHISPER_BEAM_SIZE=${WHISPER_BEAM_SIZE:-5}
      - WHISPER_VAD_FILTER=${WHISPER_VAD_FILTER:-1}
    volumes:
      - whisper_cache:/root/.cache
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: on-failure:3
    networks:
      default:
        aliases: ["whisper-stt"]

  # Wyoming OpenAI proxy - bridges Wyoming protocol to Speaches OpenAI endpoints
  wyoming_openai:
    build:
      context: ./services/wyoming_openai
    container_name: hassistant_v2_wyoming_proxy
    environment:
      - ASR_URL=http://whisper-stt:8000/v1/audio/transcriptions
      - PRIMARY_TTS_HOST=${PRIMARY_TTS_HOST:-piper-main}
      - PRIMARY_TTS_PORT=${PRIMARY_TTS_PORT:-10200}
      - FALLBACK_TTS_HOST=wyoming-piper
      - FALLBACK_TTS_PORT=10200
      - FALLBACK_TTS_URL=${FALLBACK_TTS_URL:-}
    ports:
      - "10300:10300"  # Wyoming STT (TCP bridge to Speaches)
      - "10210:10210"  # Wyoming TTS (TCP bridge to Speaches)
      - "8085:8080"    # HTTP healthcheck/config API
    depends_on:
      whisper-stt:
        condition: service_healthy
      piper-main:
        condition: service_healthy
      wyoming-piper:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: on-failure:3
    networks:
      default:
        aliases: ["wyoming-openai", "wyoming_openai"]

  piper-main:
    image: lscr.io/linuxserver/piper:gpu
    container_name: hassistant_v2_piper_main
    runtime: nvidia
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=${TZ:-Etc/UTC}
      - PIPER_VOICE=${PIPER_VOICE:-en_US-glados-medium}
      - PIPER_LENGTH=${PIPER_LENGTH:-1.0}
      - PIPER_NOISE=${PIPER_NOISE:-0.667}
      - PIPER_NOISEW=${PIPER_NOISEW:-0.8}
      - PIPER_SPEAKER=${PIPER_SPEAKER:-0}
      - PIPER_PROCS=${PIPER_PROCS:-1}
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./services/piper_data:/config
      - ./services/piper_data/custom-cont-init.d:/custom-cont-init.d
    ports:
      - "10200:10200"
    healthcheck:
      test: ["CMD-SHELL", "timeout 2 bash -c '</dev/tcp/localhost/10200' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 45s
    restart: unless-stopped
    networks:
      default:
        aliases: ["piper-main", "piper"]

  # Wyoming Piper - native TTS fallback for A/B testing (CPU for reliability)
  wyoming-piper:
    image: rhasspy/wyoming-piper:latest
    container_name: hassistant_v2_piper_fallback
    command: --voice en_US-glados-medium --data-dir /data --download-dir /data --streaming
    # No GPU - CPU fallback is more reliable for Piper ONNX
    ports:
      - "10200:10200"
    volumes:
      - ./services/piper_data:/data
      - ./services/piper_data/voices.json:/app/lib/python3.10/dist-packages/wyoming_piper/voices.json
      - ./services/piper_data/voices.json:/usr/local/lib/python3.11/dist-packages/wyoming_piper/voices.json
    healthcheck:
      test: ["CMD-SHELL", "timeout 2 bash -c '</dev/tcp/localhost/10200' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: on-failure:3
    networks:
      default:
        aliases: ["wyoming-piper"]

  mediamtx:
    image: bluenviron/mediamtx:latest
    container_name: hassistant_v2_mediamtx
    restart: unless-stopped
    network_mode: host           # simplest; or map 8554/tcp (RTSP), 8888/http
    volumes:
      - ./mediamtx.yml:/mediamtx.yml:ro

  postgres:
    image: pgvector/pgvector:pg16
    container_name: hassistant_v2_pg
    environment:
      POSTGRES_DB: glados
      POSTGRES_USER: glados
      POSTGRES_PASSWORD: glados
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./config/postgres/init.sql:/docker-entrypoint-initdb.d/00_init.sql:ro
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U glados -d glados"]
      interval: 10s
      timeout: 5s
      retries: 10
    restart: on-failure:3

  memory-migrations:
    image: pgvector/pgvector:pg16
    container_name: hassistant_v2_mem_migrations
    environment:
      PGPASSWORD: glados
    volumes:
      - ./scripts/05_memory_dedup.sql:/migrations/05_memory_dedup.sql:ro
    command:
      - psql
      - -h
      - postgres
      - -U
      - glados
      - -d
      - glados
      - -f
      - /migrations/05_memory_dedup.sql
    depends_on:
      postgres: { condition: service_healthy }
    restart: "no"

  memory-embed:
    build: { context: ./services/memory-embed }
    container_name: hassistant_v2_memembed
    environment:
      - DATABASE_URL=postgresql://glados:glados@postgres:5432/glados
      - EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - TOP_K=8
    depends_on:
      postgres: { condition: service_healthy }
      memory-migrations: { condition: service_completed_successfully }
    ports:
      - "8001:8001"
    volumes:
      - memembed_cache:/root/.cache
    healthcheck:
      test: ["CMD","curl","-fsS","http://localhost:8001/health"]
      interval: 30s
      timeout: 5s
      retries: 5
    restart: on-failure:3

  # letta-bridge:
  #   build: { context: ./services/letta-bridge }
  #   container_name: hassistant_v2_lettabridge
  #   environment:
  #     - MEMORY_URL=http://memory-embed:8001
  #     - DATABASE_URL=postgresql://glados:glados@postgres:5432/glados
  #   depends_on:
  #     memory-embed: { condition: service_healthy }
  #     postgres: { condition: service_healthy }
  #     memory-migrations: { condition: service_completed_successfully }
  #   ports:
  #     - "8010:8010"
  #   healthcheck:
  #     test: ["CMD", "curl", "-fsS", "http://localhost:8010/health"]
  #     interval: 30s
  #     timeout: 5s
  #     retries: 5
  #   restart: on-failure:3

  # GLaDOS Orchestrator - Memory-aware chat service with intelligent routing
  orchestrator:
    build: { context: ./services/glados-orchestrator }
    container_name: hassistant_v2_orchestrator
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [utility]
              device_ids: ['all']
    environment:
      # Text models (GTX 1070)
      - OLLAMA_TEXT_URL=http://ollama-text:11434
      - OLLAMA_TEXT_MODEL=qwen3:4b                # Deeper text fallback
      - OLLAMA_TEXT_MODEL_FAST=hermes3:latest     # Fast/simple queries
      # VL model (GTX 1080 Ti)
      - OLLAMA_VL_URL=http://ollama-vl:11434
      - OLLAMA_MODEL_VL=qwen2.5vl:7b              # Qwen2.5-VL 7B vision-language model
      # Router configuration
      - VL_GPU_INDEX=1                            # GTX 1080 Ti index
      - VL_IDLE_UTIL_MAX=0.50                     # 50% 5s avg => idle
      - VL_MIN_MEM_FREE_GB=3.0                    # Min free VRAM for VL
      - VL_QUEUE_MAX_WAIT_MS=400                  # Max queue wait
      - VL_TEXT_ENABLED=1                         # Enable VL for text (HA toggle)
      - VL_STICKY_TURNS_MIN=5                     # Sticky sessions on VL
      - VL_TEXT_TOKEN_LIMIT=768                   # Cap VL text tokens
      - VISION_ROUTER_URL=${VISION_ROUTER_URL:-http://192.168.122.71:8050}
      - VISION_GATEWAY_URL=${VISION_GATEWAY_URL:-http://192.168.122.71:8051}
      - HA_BASE_URL=${HA_BASE_URL:-http://assistant-ha:8123}
      - HA_TOKEN=${HA_TOKEN:-}
      # Memory configuration
      - MEMORY_URL=http://192.168.2.13:8283
      - MEMORY_AUTOSAVE_ON=1
      - MEMORY_TOP_K=6
      - MEMORY_MIN_SCORE=0.5
      - MEMORY_MAX_CTX_CHARS=1200
      - MEMORY_DURABLE_THRESHOLD=80
      - MEMORY_CACHE_SIZE=50
      - MEMORY_CONNECT_TIMEOUT=2.0
      - MEMORY_READ_TIMEOUT=6.0
      # Realtime configuration
      - ENABLE_REALTIME_STREAMING=${ENABLE_REALTIME_STREAMING:-0}
      - REALTIME_CONNECT_TIMEOUT=${REALTIME_CONNECT_TIMEOUT:-2.0}
      - REALTIME_READ_TIMEOUT=${REALTIME_READ_TIMEOUT:-30.0}
      - REALTIME_SESSION_TTL=${REALTIME_SESSION_TTL:-30.0}
    depends_on:
      ollama-text: { condition: service_healthy }
      ollama-vl: { condition: service_healthy }
      #letta-bridge: { condition: service_healthy }
      memory-embed: { condition: service_healthy }
    ports:
      - "8021:8020"
    volumes:
      - orch_state:/app/state
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8020/health')"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: on-failure:3

  memory-backfill:
    build:
      context: .
      dockerfile: services/memory-backfill/Dockerfile
    container_name: hassistant_v2_memory_backfill
    working_dir: /app
    environment:
      - DATABASE_URL=postgresql://glados:glados@postgres:5432/glados
      - MEMORY_URL=http://memory-embed:8001
    volumes:
      - ./scripts:/app/scripts
      - ./state:/app/state
      - ./v1:/app/v1:ro
      - ./sample_data:/app/sample_data:ro
    command: ["--resume", "state/backfill.json"]
    depends_on:
      memory-embed: { condition: service_healthy }
      postgres: { condition: service_healthy }
    restart: "no"

  dozzle:
    image: ghcr.io/amir20/dozzle:latest
    container_name: hassistant_v2_dozzle
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro   # read-only
    ports:
      - "9999:8080"   # host:container
    environment:
      - DOZZLE_LEVEL=info
      - DOZZLE_TAILSIZE=300
      # Optional basic auth:
      # - DOZZLE_USERNAME=admin
      # - DOZZLE_PASSWORD=yourpassword
    restart: unless-stopped
    networks:
      default:
        aliases: ["dozzle"]

  # NVIDIA GPU stats to Home Assistant via MQTT

  letta-server:
    image: lettaai/letta:latest
    container_name: hassistant_v2_letta_server
    environment:
      - POSTGRES_DB=letta
      - POSTGRES_USER=glados
      - POSTGRES_PASSWORD=glados
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - OLLAMA_BASE_URL=http://ollama-text:11434
    depends_on:
      - postgres
    ports:
      - "8083:8083"
      - "8283:8283"
    restart: on-failure:3

  smi2mqtt:
    build:
      context: ./services/smi2mqtt/smi2mqtt
    container_name: hassistant_v2_smi2mqtt
    environment:
      - TZ=EST
    volumes:
      - ./services/smi2mqtt/data:/opt/smi2mqtt
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  ollama_text_data:
    name: hassistant_v2_ollama_text_data
  ollama_vl_data:
    name: hassistant_v2_ollama_vl_data
  voices:
    name: hassistant_v2_piper_voices
  piper_data:
    name: hassistant_v2_piper_data
  whisper_cache:
    name: hassistant_v2_whisper_cache
  tailscale_state:
    name: hassistant_v2_tailscale_state
  tailscale_run:
    name: hassistant_v2_tailscale_run
  pgdata:
    name: hassistant_v2_pgdata
  memembed_cache:
    name: hassistant_v2_memembed_cache  # Cache for embedding model
  orch_state:
    name: hassistant_v2_orch_state


# ---- Step 3 (Deferred): Vision/K80 VM integration ---------------------------
# profiles:
#   - vision
#
# services:
#   vision-client:
#     # Client shim that talks to the K80 VM gateway (not local GPU)
#     image: ghcr.io/hassistant/vision-client:dev
#     environment:
#       - VISION_VM_URL=${VISION_VM_URL:-http://k80-vm:9000}
#     restart: on-failure:3
#     profiles: ["vision"]



networks:
  default:
    name: hassistant_v2_default
    external: true

