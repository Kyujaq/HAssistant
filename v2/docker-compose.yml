# HAssistant v2 - Streaming Speech Stack
# Port Allocation:
#   - 11434: ollama-chat (Hermes3 + Qwen3-4B)
#   - 10300: wyoming_openai STT (Wyoming facade)
#   - 10210: wyoming_openai TTS (proxy to Piper)
#   - 10200: piper-main (GPU-accelerated TTS)
#   - 8085:  wyoming_openai HTTP healthcheck/config API
#
# IMPORTANT: Verify GPU indices with `nvidia-smi --query-gpu=index,name --format=csv`
#            Update NVIDIA_VISIBLE_DEVICES if your GPU1 has a different index

name: hassistant_v2

services:
  # Ollama text service - hosts Hermes3 (fast) and Qwen3-4B (deeper text) on GTX 1070
  ollama-text:
    image: ollama/ollama:latest
    container_name: hassistant_v2_ollama_text
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0          # Pin to GPU0 (GTX 1070)
      - OLLAMA_KEEP_ALIVE=3h
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=1             # Avoid competing with STT bursts
    ports:
      - "11435:11434"                     # Expose on 11435 for clarity
    volumes:
      - ollama_text_data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 90s
    restart: on-failure:3
    networks:
      default:
        aliases: ["ollama-text", "ollama-chat"]  # Keep ollama-chat alias for backwards compatibility

  # Ollama VL service - hosts Qwen-VL-8B for opportunistic text escalation on GTX 1080 Ti
  ollama-vl:
    image: ollama/ollama:latest
    container_name: hassistant_v2_ollama_vl
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=1          # Pin to GPU1 (GTX 1080 Ti)
      - OLLAMA_KEEP_ALIVE=3h
      - OLLAMA_HOST=0.0.0.0:11434
    ports:
      - "11436:11434"
    volumes:
      - ollama_vl_data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 90s
    restart: on-failure:3
    networks:
      default:
        aliases: ["ollama-vl"]

  tailscale:
    image: tailscale/tailscale:latest
    container_name: hassistant_v2_tailscale
    hostname: ${TAILSCALE_HOSTNAME:-hassistant-v2}
    restart: unless-stopped
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    devices:
      - /dev/net/tun:/dev/net/tun
    volumes:
      - tailscale_state:/var/lib/tailscale
      - tailscale_run:/var/run/tailscale
    environment:
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_SOCKET=/var/run/tailscale/tailscaled.sock
    command:
      - tailscaled
      - --state=/var/lib/tailscale/tailscaled.state
      - --socket=/var/run/tailscale/tailscaled.sock
      - --tun=tsdev0
  
  mosquitto:
    image: eclipse-mosquitto:2
    container_name: hassistant_v2_mosquitto
    restart: unless-stopped
    ports:
      - "1883:1883"   # MQTT broker port
      - "9001:9001"   # WebSocket (optional)
    volumes:
      - ./mosquitto/config:/mosquitto/config
      - ./mosquitto/data:/mosquitto/data
      - ./mosquitto/log:/mosquitto/log
    networks:
      default:
        aliases: ["mosquitto"]

  whisper-stt:
    build:
      context: ./services/whisper_stt
    container_name: hassistant_v2_whisper_stt
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - USE_CUDA=1
      - WHISPER_MODEL=${WHISPER_MODEL:-base.en}
      - WHISPER_COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE:-int8_float16}
      - WHISPER_BEAM_SIZE=${WHISPER_BEAM_SIZE:-5}
      - WHISPER_VAD_FILTER=${WHISPER_VAD_FILTER:-1}
    volumes:
      - whisper_cache:/root/.cache
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    restart: on-failure:3
    networks:
      default:
        aliases: ["whisper-stt"]

  # Wyoming OpenAI proxy - bridges Wyoming protocol to Speaches OpenAI endpoints
  wyoming_openai:
    build:
      context: ./services/wyoming_openai
    container_name: hassistant_v2_wyoming_proxy
    environment:
      - ASR_URL=http://whisper-stt:8000/v1/audio/transcriptions
      - PRIMARY_TTS_HOST=${PRIMARY_TTS_HOST:-piper-main}
      - PRIMARY_TTS_PORT=${PRIMARY_TTS_PORT:-10200}
    ports:
      - "10300:10300"  # Wyoming STT (TCP bridge to Speaches)
      - "10210:10210"  # Wyoming TTS (TCP bridge to Speaches)
      - "8085:8080"    # HTTP healthcheck/config API
    depends_on:
      whisper-stt:
        condition: service_healthy
      piper-main:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: on-failure:3
    networks:
      default:
        aliases: ["wyoming-openai", "wyoming_openai"]

  piper-main:
    image: lscr.io/linuxserver/piper:gpu
    container_name: hassistant_v2_piper_main
    runtime: nvidia
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=${TZ:-Etc/UTC}
      - PIPER_VOICE=${PIPER_VOICE:-en_US-glados-medium}
      - PIPER_LENGTH=${PIPER_LENGTH:-1.0}
      - PIPER_NOISE=${PIPER_NOISE:-0.667}
      - PIPER_NOISEW=${PIPER_NOISEW:-0.8}
      - PIPER_SPEAKER=${PIPER_SPEAKER:-0}
      - PIPER_PROCS=${PIPER_PROCS:-1}
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./services/piper_data:/config
      - ./services/piper_data/custom-cont-init.d:/custom-cont-init.d
    ports:
      - "10200:10200"
    healthcheck:
      test: ["CMD-SHELL", "timeout 2 bash -c '</dev/tcp/localhost/10200' || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 45s
    restart: unless-stopped
    networks:
      default:
        aliases: ["piper-main", "piper"]

  mediamtx:
    image: bluenviron/mediamtx:latest
    container_name: hassistant_v2_mediamtx
    restart: unless-stopped
    network_mode: host           # simplest; or map 8554/tcp (RTSP), 8888/http
    volumes:
      - ./mediamtx.yml:/mediamtx.yml:ro

  postgres:
    image: pgvector/pgvector:pg16
    container_name: hassistant_v2_pg
    environment:
      TZ: ${TZ:-America/New_York}
      POSTGRES_DB: glados
      POSTGRES_USER: glados
      POSTGRES_PASSWORD: glados
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./config/postgres/init.sql:/docker-entrypoint-initdb.d/00_init.sql:ro
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U glados -d glados"]
      interval: 10s
      timeout: 5s
      retries: 10
    restart: on-failure:3

  redis:
    image: redis:7-alpine
    container_name: hassistant_v2_redis
    command: redis-server --appendonly yes --requirepass glados_redis_pass_change_me
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "glados_redis_pass_change_me", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    restart: on-failure:3

  memory-embed:
    build: { context: ./services/memory-embed }
    container_name: hassistant_v2_memembed
    environment:
      - DATABASE_URL=postgresql://glados:glados@postgres:5432/glados
      - EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - TOP_K=8
    depends_on:
      postgres: { condition: service_healthy }
    ports:
      - "8001:8001"
    volumes:
      - memembed_cache:/root/.cache
    healthcheck:
      test: ["CMD","curl","-fsS","http://localhost:8001/health"]
      interval: 30s
      timeout: 5s
      retries: 5
    restart: on-failure:3

  # letta-bridge:
  #   build: { context: ./services/letta-bridge }
  #   container_name: hassistant_v2_lettabridge
  #   environment:
  #     - MEMORY_URL=http://memory-embed:8001
  #     - DATABASE_URL=postgresql://glados:glados@postgres:5432/glados
  #   depends_on:
  #     memory-embed: { condition: service_healthy }
  #     postgres: { condition: service_healthy }
  #     memory-migrations: { condition: service_completed_successfully }
  #   ports:
  #     - "8010:8010"
  #   healthcheck:
  #     test: ["CMD", "curl", "-fsS", "http://localhost:8010/health"]
  #     interval: 30s
  #     timeout: 5s
  #     retries: 5
  #   restart: on-failure:3

  # S2P Gate - Speech-to-Phrase fast command routing
  s2p-gate:
    build: { context: ./services/s2p-gate }
    container_name: hassistant_v2_s2p_gate
    environment:
      - PHRASES_FILE=/app/phrases.yaml
      - CONFIDENCE_THRESHOLD=${S2P_CONFIDENCE_THRESHOLD:-0.85}
      - DEFAULT_LOCALE=${S2P_LOCALE:-en}
      - LOG_LEVEL=INFO
      - PORT=8083
    volumes:
      - ./services/s2p-gate/phrases.yaml:/app/phrases.yaml:ro
    ports:
      - "8083:8083"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8083/health', timeout=5)"]
      interval: 30s
      timeout: 10s
      start_period: 10s
      retries: 3
    restart: on-failure:3

  # GLaDOS Orchestrator - Memory-aware chat service with intelligent routing
  orchestrator:
    build: { context: ./services/glados-orchestrator }
    container_name: hassistant_v2_orchestrator
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [utility]
              device_ids: ['all']
    environment:
      # Timezone
      - TZ=${TZ:-America/New_York}
      # Text models (GTX 1070)
      - OLLAMA_TEXT_URL=http://ollama-text:11434
      - OLLAMA_TEXT_MODEL=qwen3:4b                # Deeper text fallback
      - OLLAMA_TEXT_MODEL_FAST=hermes3:latest     # Fast/simple queries
      # VL model (GTX 1080 Ti)
      - OLLAMA_VL_URL=http://ollama-vl:11434
      - OLLAMA_MODEL_VL=qwen2.5vl:7b              # Qwen2.5-VL 7B vision-language model
      # Router configuration
      - VL_GPU_INDEX=1                            # GTX 1080 Ti index
      - VL_IDLE_UTIL_MAX=0.50                     # 50% 5s avg => idle
      - VL_MIN_MEM_FREE_GB=3.0                    # Min free VRAM for VL
      - VL_QUEUE_MAX_WAIT_MS=400                  # Max queue wait
      - VL_TEXT_ENABLED=1                         # Enable VL for text (HA toggle)
      - VL_STICKY_TURNS_MIN=5                     # Sticky sessions on VL
      - VL_TEXT_TOKEN_LIMIT=768                   # Cap VL text tokens
      - VISION_ROUTER_URL=${VISION_ROUTER_URL:-http://192.168.122.71:8050}
      - VISION_GATEWAY_URL=${VISION_GATEWAY_URL:-http://192.168.122.71:8051}
      - HA_BASE_URL=https://hassistant-homeassistant:8123
      - HA_TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiI5MDI5ZTA1YTIzMTM0ZDUxOTQ4MjAzZjAzYjVjM2U2YyIsImlhdCI6MTc2MTE3ODczMSwiZXhwIjoyMDc2NTM4NzMxfQ.GcPdOJZrfQ0rtRRj2hv9IgXM-gVQH0cKw7uV2Cd_8hA
      # Memory configuration
      - MEMORY_URL=http://192.168.2.13:8283
      - MEMORY_AUTOSAVE_ON=1
      - MEMORY_TOP_K=6
      - MEMORY_MIN_SCORE=0.5
      - MEMORY_MAX_CTX_CHARS=1200
      - MEMORY_DURABLE_THRESHOLD=80
      - MEMORY_CACHE_SIZE=50
      - MEMORY_CONNECT_TIMEOUT=2.0
      - MEMORY_READ_TIMEOUT=6.0
      # Letta configuration
      - LETTA_SERVER_URL=http://letta-server:8283
      - LETTA_API_KEY=${LETTA_API_KEY}
      - LETTA_AGENT_ID=${LETTA_AGENT_ID}
      - ORCH_BASE_URL=http://orchestrator:8082
      # S2P configuration
      - S2P_URL=http://s2p-gate:8083
      - S2P_CONFIDENCE_THRESHOLD=${S2P_CONFIDENCE_THRESHOLD:-0.85}
      - S2P_LOCALE=${S2P_LOCALE:-en}
      # Redis configuration
      - "REDIS_URL=redis://:glados_redis_pass_change_me@redis:6379"
      - "REDIS_FACTS_PREFIX=facts:"
      # Parallel ack configuration
      - ACK_MODEL=hermes3:latest
      - ACK_MAX_TOKENS=64
      - ACK_TIMEOUT_MS=3000
      # Realtime/TTS configuration
      - ENABLE_REALTIME_STREAMING=${ENABLE_REALTIME_STREAMING:-0}
      - REALTIME_CONNECT_TIMEOUT=${REALTIME_CONNECT_TIMEOUT:-2.0}
      - REALTIME_READ_TIMEOUT=${REALTIME_READ_TIMEOUT:-30.0}
      - REALTIME_SESSION_TTL=${REALTIME_SESSION_TTL:-30.0}
      - BARGE_IN_ENABLED=true
      - BARGE_IN_DEBOUNCE_MS=150
      - "REALTIME_FLUSH_MARKERS=.?!\\n"
    depends_on:
      ollama-text:
        condition: service_healthy
      ollama-vl:
        condition: service_healthy
      memory-embed:
        condition: service_healthy
      s2p-gate:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "8082:8020"
    volumes:
      - orch_state:/app/state
      - ./config/ha/entities.yaml:/app/config/ha/entities.yaml:ro
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8020/health')"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: on-failure:3

  dozzle:
    image: ghcr.io/amir20/dozzle:latest
    container_name: hassistant_v2_dozzle
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro   # read-only
    ports:
      - "9999:8080"   # host:container
    environment:
      - DOZZLE_LEVEL=info
      - DOZZLE_TAILSIZE=300
      # Optional basic auth:
      # - DOZZLE_USERNAME=admin
      # - DOZZLE_PASSWORD=yourpassword
    restart: unless-stopped
    networks:
      default:
        aliases: ["dozzle"]


  letta-server:
    image: letta/letta:latest
    container_name: hassistant_v2_letta_server
    restart: unless-stopped
    environment:
      - TZ=${TZ:-America/New_York}
      - LETTA_API_KEY=${LETTA_API_KEY}
      - HA_BASE_URL=http://192.168.2.13:8123
      - HA_TOKEN=${HA_TOKEN}
      - ORCH_BASE_URL=http://glados-orchestrator:8082
      - SEARX_URL=http://searxng:8084/search
      - READABILITY_URL=http://readability:8086/read
      - DOC_INDEX_URL=http://doc-index:8087/search
    volumes:
      - /home/qjaq/.letta/.persist/pgdata:/var/lib/postgresql/data
    ports:
      - "8081:8283"
    networks:
      - default

  # NVIDIA GPU stats to Home Assistant via MQTT
  smi2mqtt:
    build:
      context: ./services/smi2mqtt/smi2mqtt
    container_name: hassistant_v2_smi2mqtt
    environment:
      - TZ=${TZ:-America/New_York}
    volumes:
      - ./services/smi2mqtt/data:/opt/smi2mqtt
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  prometheus:
    image: prom/prometheus:latest
    container_name: hassistant_v2_prometheus
    restart: unless-stopped
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/alerting/alerts.yml:/etc/prometheus/alerts.yml:ro
    ports:
      - "9090:9090"
    networks: [ default ]

  grafana:
    image: grafana/grafana:latest
    container_name: hassistant_v2_grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=glados
      - GF_SERVER_DOMAIN=localhost
    depends_on: [ prometheus ]
    ports:
      - "3000:3000"
    volumes:
      - ./config/grafana/provisioning/:/etc/grafana/provisioning/:ro
      - ./config/grafana/dashboards/:/var/lib/grafana/dashboards/:ro
    networks: [ default ]

  alertmanager:
    image: prom/alertmanager:latest
    container_name: hassistant_v2_alertmanager
    restart: unless-stopped
    command: ["--config.file=/etc/alertmanager/alertmanager.yml"]
    ports:
      - "9093:9093"
    volumes:
      - ./config/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    networks: [ default ]
    depends_on: [ prometheus ]

  searxng:
    image: searxng/searxng:latest
    container_name: hassistant_v2_searxng
    restart: unless-stopped
    environment:
      - BASE_URL=http://searxng:8084
      - SEARXNG_SETTINGS_PATH=/etc/searxng/settings.yml
    volumes:
      - ./config/searxng/settings.yml:/etc/searxng/settings.yml:ro
    ports:
      - "8084:8080"
    networks: [ default ]

  readability:
    build:
      context: ./services/readability
    container_name: hassistant_v2_readability
    restart: unless-stopped
    environment:
      - WEB_ALLOWLIST=${WEB_ALLOWLIST}
      - READABILITY_TIMEOUT_MS=5000
      - READABILITY_SIZE_LIMIT=1500000
    ports:
      - "8086:8086"
    networks: [ default ]

  doc-index:
    build:
      context: ./services/doc-index
    container_name: hassistant_v2_docindex
    restart: unless-stopped
    environment:
      - DOC_INDEX_DB=${DOC_INDEX_DB}
      - DOC_INDEX_PATHS=${DOC_INDEX_PATHS}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
    volumes:
      - /home/qjaq/Documents:/mnt/docs:ro
      - ./data/docindex:/var/lib/docindex
    ports:
      - "8087:8087"
    networks: [ default ]

volumes:
  ollama_text_data:
    name: hassistant_v2_ollama_text_data
  ollama_vl_data:
    name: hassistant_v2_ollama_vl_data
  voices:
    name: hassistant_v2_piper_voices
  piper_data:
    name: hassistant_v2_piper_data
  whisper_cache:
    name: hassistant_v2_whisper_cache
  tailscale_state:
    name: hassistant_v2_tailscale_state
  tailscale_run:
    name: hassistant_v2_tailscale_run
  pgdata:
    name: hassistant_v2_pgdata
  redis_data:
    name: hassistant_v2_redis_data
  memembed_cache:
    name: hassistant_v2_memembed_cache  # Cache for embedding model
  orch_state:
    name: hassistant_v2_orch_state


# ---- Step 3 (Deferred): Vision/K80 VM integration ---------------------------
# profiles:
#   - vision
#
# services:
#   vision-client:
#     # Client shim that talks to the K80 VM gateway (not local GPU)
#     image: ghcr.io/hassistant/vision-client:dev
#     environment:
#       - VISION_VM_URL=${VISION_VM_URL:-http://k80-vm:9000}
#     restart: on-failure:3
#     profiles: ["vision"]



networks:
  default:
    name: hassistant_v2_default
    external: true

