services:
  # Disabled old ollama service - using ollama-chat and ollama-vision instead
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: hassistant-ollama
  #   runtime: nvidia
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['0', '1']  # Both GTX 1080 Ti and GTX 1070
  #             capabilities: [gpu]
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - OLLAMA_HOST=0.0.0.0:11434
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ./ollama:/root/.ollama  # Persistent model storage
  #     - /home/qjaq/assistant/models:/models:ro  # Read-only access to existing models
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 10s
  #   networks:
  #     - ha_network

  # Wyoming Whisper STT (Speech-to-Text) - GPU Accelerated with CUDA/cuDNN support
  whisper:
    image: slackr31337/wyoming-whisper-gpu:latest
    container_name: hassistant-whisper
    runtime: nvidia
    command: --model small --language en --uri 'tcp://0.0.0.0:10300' --data-dir /data --device cuda
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['1']  # GPU 1 → GTX 1070 (lighter model, good for STT)
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
    volumes:
      - ./whisper_data:/data
    ports:
      - "10300:10300"
    restart: unless-stopped
    networks:
      - ha_network

  # Wyoming Piper TTS (Text-to-Speech) - GLaDOS voice - GPU Accelerated
  piper-glados:
    image: rhasspy/wyoming-piper:latest
    container_name: hassistant-piper-glados
    runtime: nvidia
    command: --voice en_US-glados-medium --uri 'tcp://0.0.0.0:10200' --data-dir /data --piper /usr/share/piper/piper --streaming
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['1']  # GPU 1 → GTX 1070 (share with Whisper)
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
    volumes:
      - ./piper_data:/data
    ports:
      - "10200:10200"
    restart: unless-stopped
    networks:
      - ha_network

  # Windows Voice Control Bridge
  # Sends commands to Windows laptop via audio cable using Piper TTS
  windows-voice-control:
    build:
      context: .
      dockerfile: services/windows-voice-control/Dockerfile
    container_name: hassistant-windows-voice-control
    environment:
      - TTS_URL=http://hassistant-piper-glados:10200
      - PIPER_HOST=hassistant-piper-glados
      - PIPER_PORT=10200
      - USB_AUDIO_DEVICE=${USB_AUDIO_DEVICE:-hw:5,0}  # Card 5 = USB Audio Device
      - USE_PULSEAUDIO=${USE_PULSEAUDIO:-false}
      - PULSEAUDIO_SINK=${PULSEAUDIO_SINK:-alsa_output.usb-default}
      - WYOMING_ENABLED=${WYOMING_ENABLED:-false}
      - USE_DIRECT_PIPER=false  # Don't use direct Piper in Docker, use Wyoming protocol
    devices:
      - /dev/snd:/dev/snd  # Pass through audio devices
    volumes:
      - ./clients/windows_voice_control.py:/app/windows_voice_control.py:ro
    restart: unless-stopped
    networks:
      - ha_network
    depends_on:
      - piper-glados
    profiles:
      - windows-control  # Optional service, enable with --profile windows-control

  # Home Assistant
  homeassistant:
    image: ghcr.io/home-assistant/home-assistant:stable
    container_name: hassistant-homeassistant
    volumes:
      - /home/qjaq/assistant/data/ha_config:/config
      - /etc/localtime:/etc/localtime:ro
    ports:
      - "8123:8123"
    restart: unless-stopped
    privileged: true
    networks:
      - ha_network

  # PostgreSQL with pgvector for Letta memory
  postgres:
    image: pgvector/pgvector:pg15
    container_name: hassistant-postgres
    environment:
      - POSTGRES_DB=hassistant
      - POSTGRES_USER=hassistant
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-postgres}
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts:/docker-entrypoint-initdb.d:ro
    ports:
      - "5432:5432"  # Use 5433 to avoid conflict with /assistant postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hassistant -d hassistant"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - ha_network

  # Redis for session cache and ephemeral data
  redis:
    image: redis:7-alpine
    container_name: hassistant-redis
    command: redis-server --appendonly yes --requirepass :gp9U0%o5>~Y1~'@Ntz528B(.£L4N,Qq59eE
    volumes:
      - redis_data:/data
    ports:
      - "6380:6379"  # Use 6380 to avoid conflict with /assistant redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-CHANGE_ME_STRONG_PASSWORD_REQUIRED}", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - ha_network

  # Letta Bridge API
  letta-bridge:
    build: ./services/letta-bridge
    container_name: hassistant-letta-bridge
    env_file: .env
    restart: unless-stopped
    # FIX 1: Map the host port 8081 to the container's internal port 8000
    ports:
      - "8081:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      # FIX 2: The healthcheck must use the internal port 8000
      test: ["CMD", "curl", "-f", "-H", "x-api-key: ${BRIDGE_API_KEY:-dev-key}", "http://localhost:8000/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ha_network

  # PC Control Agent for AI orchestration
 # pc-control-agent:
  #  build:
   #   context: ./services/pc-control-agent
    #  dockerfile: Dockerfile
   # container_name: hassistant-pc-control
   # env_file: .env
   # depends_on:
   #   - letta-bridge
   # restart: unless-stopped
   # networks:
   #   - ha_network
    # Optional: mount your data/tools/configs
   # volumes:
   #   - ./agent_data:/data

  # GLaDOS Orchestrator v2.1 - Tool Provider + Smart Routing
  glados-orchestrator:
      build:
        context: ./services/glados-orchestrator
        dockerfile: Dockerfile
      container_name: hassistant-glados-orchestrator
      environment:
        - LETTA_BRIDGE_URL=http://hassistant-letta-bridge:8000
        - LETTA_API_KEY=${BRIDGE_API_KEY:-dev-key}
        - PORT=8082
        # Smart routing configuration
        - OLLAMA_CHAT_URL=http://ollama-chat:11434
        - OLLAMA_VISION_URL=http://ollama-vision:11434
        - HERMES_MODEL=glados-hermes3
        - QWEN_MODEL=qwen3:4b-instruct-2507-q4_K_M
        - VISION_MODEL=qwen2.5vl:7b
      ports:
        - "8082:8082"
      depends_on:
        - letta-bridge
        - ollama-chat
        - ollama-vision
      restart: unless-stopped
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8082/healthz"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 30s
      networks:
        - ha_network

  # ---- Vision Gateway (K80 GPU for continuous object detection) ----
  vision-gateway:
    build:
      context: ./services/vision-gateway
      dockerfile: Dockerfile
    container_name: vision-gateway
    runtime: nvidia
    privileged: true  # Required for V4L2 device access
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['2']   # GPU 2 → Tesla K80 (24GB, for GroundingDINO)
    environment:
      - NVIDIA_VISIBLE_DEVICES=2
      # HA push
      - HA_BASE_URL=http://homeassistant:8123
      - HA_TOKEN=${HA_TOKEN}
      # VL endpoint (1070 / Ollama-vision)
      - OLLAMA_VISION_BASE=http://ollama-vision:11434
      - OLLAMA_VISION_MODEL=qwen2.5vl:7b
      # K80 GPU preprocessing
      - K80_ENABLED=true  # K80 continuous detection ENABLED!
      - K80_DEVICE=cuda:2
      - K80_BOX_THRESHOLD=0.35
      - K80_TEXT_THRESHOLD=0.25
      - K80_SCENE_CHANGE_THRESHOLD=0.3
      # Direct HDMI capture from UGREEN dongle
      - HDMI_ENABLED=true
      - HDMI_DEVICE=/dev/video2
      - HDMI_WIDTH=1920
      - HDMI_HEIGHT=1080
      - HDMI_CAP_FPS=12
      - HDMI_FORCE_MJPG=false
      # scan resolution
      #- SCAN_WIDTH=960
      #- SCAN_HEIGHT=540
      - MATCH_EVERY_N=2  
      # Motion & debounce
      - MOTION_THRESHOLD=0.015
      - COOLDOWN_SECONDS=8
      # Anchor-based detection configuration
      - OCR_MODE=anchor_based  # anchor_based or full_screen
      - ANCHOR_KEYWORDS=Accept,Send,Join,Decline  # Action buttons only (removed Meeting,Invite to reduce false positives)
      - CONTEXT_ZONES_ENABLED=true
      - BUTTON_COORDS=45,350,90,114
      - BUTTON_THRESH=0.40
      - PRESSED_THRESH=0.55
      - PRESS_MARGIN=0.04
      - MIN_PRESSED=0.50
      - DISAPPEAR_THRESH=0.35
      - DISAPPEAR_FRAMES=3
      - PRESS_TIMEOUT_S=3.0
      - REARM_COOLDOWN=1.5
      - HDMI_RESIZE_LONG=1280

    volumes:
      - ./services/vision-gateway/cache:/app/cache
      - ./services/vision-gateway/app/assets:/app/assets:ro   # <-- host ./assets → container /app/assets, read-only
      - ./services/vision-gateway/models:/app/models   # <-- K80 model weights (persistent)
    devices:
      - /dev/video2:/dev/video2  # UGREEN HDMI capture dongle
    ports:
      - "8088:8088"  # Debug UI at http://localhost:8088/debug
    depends_on:
      - homeassistant
    restart: unless-stopped
    networks:
      - ha_network

  # ---- Real-World Vision Gateway (K80 GPU 3 for people/face/pose detection) ----
  realworld-gateway:
    build:
      context: ./services/realworld-gateway
      dockerfile: Dockerfile
    container_name: realworld-gateway
    runtime: nvidia
    privileged: true  # Required for V4L2 device access
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['3']   # GPU 3 → Tesla K80 #2 (24GB, for YOLOv8/RetinaFace/MediaPipe)
    environment:
      - NVIDIA_VISIBLE_DEVICES=3
      # HA push
      - HA_BASE_URL=http://homeassistant:8123
      - HA_TOKEN=${HA_TOKEN}
      # VL endpoint (GPU 0 / Ollama-vision)
      - OLLAMA_VISION_BASE=http://ollama-vision:11434
      - OLLAMA_VISION_MODEL=qwen2.5vl:7b
      # CompreFace for face recognition
      - COMPREFACE_URL=http://compreface-api:8000
      - COMPREFACE_API_KEY=${COMPREFACE_API_KEY:-}
      # Frigate integration
      - FRIGATE_MODE=true  # Consume Frigate snapshots instead of direct webcam
      - FRIGATE_URL=http://frigate:5000
      - FRIGATE_CAMERA=webcam
      - FRIGATE_POLL_INTERVAL=2.0
      # K80 GPU 3 preprocessing
      - K80_ENABLED=true  # K80 continuous detection ENABLED!
      - K80_DEVICE=cuda:3  # GPU 3 (Tesla K80 #2)
      - K80_SCENE_CHANGE_THRESHOLD=0.3
      # Webcam capture (disabled - using Frigate)
      - WEBCAM_ENABLED=false
      # Detection processing
      - PROCESS_EVERY_N=3  # Process every 3rd frame
    volumes:
      - ./services/realworld-gateway/models:/app/models   # Model weights (persistent)
    # devices:  # Removed - Frigate accesses webcam, we get snapshots from Frigate
    #   - /dev/video0:/dev/video0
    ports:
      - "8089:8089"  # Debug UI at http://localhost:8089/debug
    depends_on:
      - homeassistant
      - ollama-vision
      - frigate  # Get snapshots from Frigate
    restart: unless-stopped
    networks:
      - ha_network

  # ---- Frigate (webcam motion, snapshots) ----
  frigate:
    image: ghcr.io/blakeblackshear/frigate:stable
    container_name: frigate
    runtime: nvidia
    privileged: true
    shm_size: "512m"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu,video]
              device_ids: ['3']   # GPU 3 → K80 #2 (shared with realworld-gateway)
    environment:
      - FRIGATE_RTSP_PASSWORD=${FRIGATE_RTSP_PASSWORD:-CHANGE_ME_FRIGATE_PASSWORD}
      - NVIDIA_VISIBLE_DEVICES=3
    volumes:
      - ./frigate/config:/config
      - /etc/localtime:/etc/localtime:ro
      - ./frigate/media:/media/frigate
    devices:
      - /dev/video0:/dev/video0
      - /dev/video2:/dev/video2

    ports:
      - "5000:5000"     # Frigate UI/API
      - "8554:8554"     # RTSP
      - "8555:8555/tcp" # WebRTC
      - "8555:8555/udp" # WebRTC
      - "1984:1984"     # go2rtc API
    restart: unless-stopped
    networks:
      - ha_network

  # Optional helper to snapshot webcam via Frigate API (no GPU)
  frigate-snapshotper:
    image: curlimages/curl:8.10.1
    container_name: frigate-snapshotper
    command: "sleep infinity"
    networks:
      - ha_network

  # Your ollama containers should already exist:
  # - ollama-chat on GPU0 (11434) with Qwen3-* for chat
  # - ollama-vision on GPU1 (11435->11434 in container) with qwen2.5vl:7b
  # Ensure they're reachable as ollama-chat / ollama-vision or use host IPs.
  
  ollama-chat:
    image: ollama/ollama:latest
    container_name: ollama-chat
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['0']   # GPU 0 → 1080 Ti
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - NVIDIA_VISIBLE_DEVICES=0
      - OLLAMA_KEEP_ALIVE=-1  # Keep models loaded indefinitely
    volumes:
      - ollama_chat_data:/root/.ollama
      - ./ollama/modelfiles:/root/.ollama/modelfiles:ro
      - /home/qjaq/assistant/models:/models:ro
    restart: unless-stopped
    networks:
      - ha_network

  ollama-vision:
    image: ollama/ollama:latest
    container_name: ollama-vision
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['0']   # GPU 0 → GTX 1080 Ti (11GB - fits 8.6GB model)
    ports:
      - "11435:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - NVIDIA_VISIBLE_DEVICES=1
    volumes:
      - ollama_vision_data:/root/.ollama
      - ./ollama/modelfiles:/root/.ollama/modelfiles:ro
      - /home/qjaq/assistant/models:/models:ro
    restart: unless-stopped
    networks:
      - ha_network
  
   # ---- NEW: Face Recognition Stack ---- #

  # Double Take: Orchestrates face recognition between Frigate and CompreFace
  double-take:
    container_name: double-take
    image: jakowenko/double-take
    restart: unless-stopped
    ports:
      - 3000:3000 # Web UI for Double Take
    volumes:
      - ./double-take-config:/config
      - ./double-take-media:/media
    networks:
      - ha_network

  # CompreFace API: The main API service for face recognition
  compreface-api:
    container_name: compreface-api
    image: exadel/compreface-api:1.1.0
    restart: unless-stopped
    mem_limit: 1g
    ports:
      - "8000:8000"
    volumes:
      - compreface_db_storage:/var/lib/postgresql/data
    depends_on:
      compreface-postgres:
        condition: service_healthy
    environment:
      POSTGRES_URL: "jdbc:postgresql://${COMPREFACE_POSTGRES_USER:-postgres}:${COMPREFACE_POSTGRES_PASSWORD:-postgres}@compreface-postgres-db:5432/frs"
      API_JAVA_OPTS: "-Xmx768m"
    networks:
      - ha_network

  # CompreFace Admin: The UI for managing faces and training the model
  compreface-admin:
    container_name: compreface-admin
    image: exadel/compreface-admin:1.1.0
    restart: unless-stopped
    environment:
      DB_HOST: compreface-postgres
      DB_USER: ${COMPREFACE_POSTGRES_USER:-postgres}
      DB_NAME: frs
      DB_PASSWORD: ${COMPREFACE_POSTGRES_PASSWORD:-postgres}
    ports:
      - "81:8080"
    depends_on:
      compreface-postgres:
        condition: service_healthy
    networks:
      - ha_network

  # CompreFace Core: The actual face detection/recognition ML model
  compreface-core:
    container_name: compreface-core
    image: exadel/compreface-core:1.1.0
    restart: unless-stopped
    environment:
      - ML_PORT=3000
      - SAVE_FACES_TO_DB=true
    # To enable GPU acceleration for CompreFace, uncomment the deploy section below
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    networks:
      - ha_network

  # CompreFace PostgreSQL Database: A dedicated DB for CompreFace
  compreface-postgres:
    container_name: compreface-postgres-db
    image: postgres:13.1
    restart: unless-stopped
    ports:
      - "5433:5432"
    environment:
      - POSTGRES_USER=${COMPREFACE_POSTGRES_USER:-postgres}
      - POSTGRES_PASSWORD=${COMPREFACE_POSTGRES_PASSWORD:-postgres}
      - POSTGRES_DB=frs
    volumes:
      - compreface_db_storage:/var/lib/postgresql/data
    networks:
      - ha_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${COMPREFACE_POSTGRES_USER:-postgres} -d frs"]
      interval: 10s
      timeout: 5s
      retries: 5

  
  # Computer Control Agent (optional, for remote computer automation)
  # Uncomment to enable computer control capabilities
  # computer-control-agent:
  #   build:
  #     context: ./clients
  #     dockerfile: Dockerfile.computer_control
  #   container_name: hassistant-computer-control
  #   environment:
  #     - VISION_GATEWAY_URL=http://vision-gateway:8088
  #     - OLLAMA_URL=http://ollama-chat:11434
  #     - OLLAMA_MODEL=qwen3:4b-instruct-2507-q4_K_M
  #     - HA_URL=http://homeassistant:8123
  #     - HA_TOKEN=${HA_TOKEN}
  #     - CONFIRM_BEFORE_ACTION=true
  #     - MAX_ACTIONS_PER_TASK=50
  #   volumes:
  #     - ./config/computer_control_agent.env:/app/computer_control_agent.env
  #     - /tmp/.X11-unix:/tmp/.X11-unix  # For X11 forwarding
  #   depends_on:
  #     - vision-gateway
  #     - ollama-chat
  #   restart: unless-stopped
  #   networks:
  #     - ha_network

#Crew Orchestrator - Multi-Agent UI Automation (NEW!)
#   Uses CrewAI to plan, execute, and verify UI automation tasks
#  Uncomment to enable crew-based automation
  crew-orchestrator:
     build:
       context: ./services/crew-orchestrator
       dockerfile: Dockerfile
     container_name: hassistant-crew-orchestrator
     environment:
       - PORT=8085
       - VISION_GATEWAY_URL=http://vision-gateway:8088
       - WINDOWS_VOICE_CONTROL_URL=http://localhost:8085
       - PYTHONPATH=/shared
       - OPENAI_API_BASE=${OPENAI_API_BASE:-http://ollama-chat:11434/v1}
       - OPENAI_API_KEY=${OPENAI_API_KEY:-sk-local}
       - OPENAI_MODEL=${OPENAI_MODEL:-qwen3:4b-instruct-2507-q4_K_M}
       # Windows Voice Control settings
       - USE_DIRECT_PIPER=false
       - WYOMING_ENABLED=false
       - TTS_URL=http://hassistant-piper-glados:5000
       - PIPER_HOST=hassistant-piper-glados
       - PIPER_PORT=5000
       - USB_AUDIO_DEVICE=hw:5,0
     volumes:
       - ./shared:/shared:ro  # Shared modules (voice, vision clients)
       - ./clients:/clients:ro  # Windows voice control script
     ports:
       - "8085:8085"
     depends_on:
       - vision-gateway
       - ollama-chat
     restart: unless-stopped
     healthcheck:
       test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:8085/healthz', timeout=5).raise_for_status()"]
       interval: 30s
       timeout: 10s
       retries: 3
       start_period: 15s
     networks:
       - ha_network

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  ollama_chat_data:
    driver: local
  ollama_vision_data:
    driver: local
  piper_data:
  whisper_data:
  ha_config:
  mosquitto_config:
  mosquitto_data:
  mosquitto_log:
  agent_data:
  compreface_db_storage:
  double-take-config:
  double-take-media:

networks:
  ha_network:
    external: true
    name: assistant_default  # Connect to existing HA network
