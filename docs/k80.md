# Tesla K80 GPU Integration Guide

This guide covers the setup, configuration, and troubleshooting of NVIDIA Tesla K80 GPUs for vision preprocessing and detection workloads in HAssistant.

## Overview

The Tesla K80 is a dual-GPU accelerator based on the Kepler GK210 architecture. Each K80 board contains two GK210 GPUs, providing:

- **24 GB total VRAM** (12 GB per GPU)
- **Compute Capability 3.7** (Kepler architecture)
- **CUDA support up to 11.4** (last version with full Kepler support)
- **No display output** - headless operation only
- **No NVENC** - dedicated for compute workloads

## Architecture

```
┌─────────────────────────────────────────────┐
│           HAssistant GPU Layout             │
├─────────────────────────────────────────────┤
│ GPU 0: GTX 1080 Ti (11GB)                   │
│   - ollama-vision (Qwen-VL 7B)              │
│   - ollama-chat (Qwen3)                     │
├─────────────────────────────────────────────┤
│ GPU 1: GTX 1070 (8GB)                       │
│   - Whisper STT                             │
│   - Piper TTS (GLaDOS)                      │
│   - Frigate (motion detection)              │
├─────────────────────────────────────────────┤
│ GPU 2: Tesla K80 #0 (12GB) - NEW            │
│   - vision-screen worker                    │
│   - OpenCV CUDA preprocessing               │
│   - YOLOv8n detection                       │
│   - OCR crops (CPU)                         │
├─────────────────────────────────────────────┤
│ GPU 3: Tesla K80 #1 (12GB) - NEW            │
│   - vision-room worker                      │
│   - Webcam preprocessing                    │
│   - Object detection                        │
│   - Face detection prep                     │
└─────────────────────────────────────────────┘
```

## Requirements

### Hardware
- NVIDIA Tesla K80 (single or dual board)
- PCIe x16 slot (x8 electrical minimum)
- **Power**: 300W per K80 board (2x 8-pin PCIe power connectors)
- **Cooling**: Passive heatsink - requires active airflow
  - Recommended: 2x 120mm fans blowing along card length
  - Maintain intake air temp < 35°C
  - Target GPU temp < 80°C under load

### Software
- **Host driver**: NVIDIA driver 470.x - 515.x series
  - Latest: `nvidia-driver-515` (Ubuntu/Debian)
  - Check compatibility: `ubuntu-drivers devices`
- **CUDA Runtime**: 11.4.x (in containers)
- **Docker**: 20.10+
- **NVIDIA Container Toolkit**: Latest version

## Installation

### 1. Install Host Driver

```bash
# Ubuntu/Debian
sudo ubuntu-drivers autoinstall
# or manually:
sudo apt install nvidia-driver-515

# Verify installation
nvidia-smi
```

Expected output should show both K80 GPUs:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.xx.xx    Driver Version: 515.xx.xx    CUDA Version: 11.7   |
|-------------------------------+----------------------+----------------------+
|   2  Tesla K80           Off  | 00000000:03:00.0 Off |                    0 |
|   3  Tesla K80           Off  | 00000000:04:00.0 Off |                    0 |
+-----------------------------------------------------------------------------+
```

### 2. Verify GPU Detection

```bash
# Detect K80 GPUs
make k80-detect

# Expected output:
# 2, Tesla K80, 11441 MiB, 3.7
# 3, Tesla K80, 11441 MiB, 3.7
```

### 3. Configure Environment

Copy and edit `.env`:

```bash
cp config/.env.example .env
nano .env
```

Add/verify K80 configuration:

```bash
# Tesla K80 GPU assignments
VISION_SCREEN_CUDA_DEVICE=2  # First K80 GPU
VISION_ROOM_CUDA_DEVICE=3    # Second K80 GPU
```

### 4. Build and Test

```bash
# Build vision-worker image
make k80-build

# Run warmup tests
make k80-warmup

# Run 10-minute burn-in test
make k80-burnin
```

### 5. Start Services

```bash
# Start both vision workers
make k80-up

# Check health
make k80-health

# Follow logs
make k80-logs
```

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `VISION_SCREEN_CUDA_DEVICE` | `2` | GPU index for screen worker |
| `VISION_ROOM_CUDA_DEVICE` | `3` | GPU index for room worker |
| `VISION_CUDA_DEVICE` | - | Set per container (auto from above) |
| `VISION_ROLE` | - | Worker role: `screen` or `room` |
| `PORT` | `8089`/`8090` | HTTP API port |

### GPU Assignment Modes

#### Static Assignment (Recommended)
```bash
# docker-compose.yml or .env
VISION_SCREEN_CUDA_DEVICE=2
VISION_ROOM_CUDA_DEVICE=3
```

#### Auto-Discovery (Advanced)
The workers can auto-detect K80 GPUs:
```bash
# If VISION_CUDA_DEVICE not set, falls back to CPU
# To enable auto-discovery, modify docker-compose to:
#   VISION_CUDA_DEVICE=${VISION_SCREEN_CUDA_DEVICE:-auto}
```

#### CPU Fallback
If GPU not available, workers run in CPU-only mode with warnings:
```
⚠️  CUDA not available. Running in CPU-only mode.
```

## Performance Optimization

### Enable Persistence Mode
Reduces driver initialization latency:
```bash
sudo nvidia-smi -pm 1
```

### Disable ECC (Optional)
Gains ~10% VRAM and slight performance boost (safe for vision workloads):
```bash
# Disable ECC on both K80 GPUs
sudo nvidia-smi -i 2 --ecc-config=0
sudo nvidia-smi -i 3 --ecc-config=0

# Reboot required for ECC changes to take effect
sudo reboot
```

⚠️ **Note**: Disabling ECC is safe for non-critical vision workloads but not recommended for scientific computing.

### Power Limit Tuning
K80 default is 150W per GPU. Can increase for better performance:
```bash
# Set to max (300W per board)
sudo nvidia-smi -i 2 -pl 300
sudo nvidia-smi -i 3 -pl 300
```

### Cooling Setup
K80 is passively cooled and requires good airflow:

1. **Fan placement**: 2x 120mm fans in front of card
2. **Airflow direction**: Front → Back (along heatsink fins)
3. **Fan speed**: 1500-2000 RPM minimum
4. **Case ventilation**: Ensure rear exhaust is clear

Target temperatures:
- **Idle**: 30-40°C
- **Load**: 70-80°C
- **Max**: 82°C (thermal throttling threshold)

## API Reference

### Health Check
```bash
curl http://localhost:8089/health | jq
```

Response:
```json
{
  "ok": true,
  "role": "screen",
  "gpu_name": "Tesla K80",
  "gpu_index": 2,
  "device": "cuda:2",
  "temp_c": 72.0,
  "util_pct": 85.0,
  "vram_used_mb": 4096.0,
  "fps_preproc": 45.2,
  "fps_detector": 28.5,
  "ocr_ms": 125.0,
  "warmup_completed": true
}
```

### Process Frame
```bash
curl -X POST http://localhost:8089/process/frame \
  -F "file=@/path/to/frame.jpg" | jq
```

Response:
```json
{
  "ok": true,
  "detections": [
    {
      "bbox": [100, 200, 50, 80],
      "conf": 0.85,
      "class": 0
    }
  ],
  "gpu": "Tesla K80",
  "device": "cuda:2"
}
```

### OCR Crop
```bash
curl -X POST http://localhost:8089/ocr/crop \
  -F "file=@/path/to/crop.jpg" | jq
```

Response:
```json
{
  "ok": true,
  "text_lines": [
    {"text": "Hello World", "conf": 0.95}
  ],
  "ocr_ms": 120.0
}
```

## Monitoring

### Real-time GPU Stats
```bash
# Watch GPU utilization
make k80-stats

# Or manually:
watch -n 1 nvidia-smi --query-gpu=index,name,temperature.gpu,utilization.gpu,memory.used --format=csv
```

### Docker Logs
```bash
# Follow both workers
make k80-logs

# Individual workers
docker logs -f vision-worker-screen
docker logs -f vision-worker-room
```

### Structured Logs
Workers emit JSON-formatted logs:
```json
{
  "timestamp": "2024-01-15T10:30:45",
  "level": "INFO",
  "service": "vision-worker",
  "role": "screen",
  "message": "Warmup complete",
  "gpu_index": 2,
  "gpu_name": "Tesla K80",
  "fps_preproc": 45.2,
  "fps_detector": 28.5,
  "ocr_ms": 125.0,
  "gpu_temp": 72.0,
  "gpu_util": 85.0
}
```

## Testing

### Warmup Test
Quick validation of GPU functionality:
```bash
make k80-warmup
```

Runs:
1. OpenCV CUDA operations (resize, color conversion)
2. YOLO detector inference
3. OCR test crop

Expected output:
```
✅ GPU 2 initialized: Tesla K80
✅ OpenCV CUDA warmup: 45.2 FPS
✅ Detector warmup: 28.5 FPS
✅ OCR warmup: 125.0 ms
✅ Warmup complete
```

### Burn-in Test
10-minute stress test to validate stability:
```bash
make k80-burnin
```

Test validates:
- No thermal throttling
- Sustained 80-95% GPU utilization
- Temperature stays < 82°C
- No driver errors or crashes

### Health Checks
Automated health checks run every 30s in docker-compose:
```bash
# Manual health check
curl http://localhost:8089/health
curl http://localhost:8090/health
```

## Troubleshooting

### Issue: "No K80 GPUs detected"

**Cause**: Driver not installed or K80 not seated properly

**Solution**:
```bash
# Check PCI devices
lspci | grep -i nvidia

# Reinstall driver
sudo apt purge nvidia-*
sudo ubuntu-drivers autoinstall
sudo reboot

# Check physical connection
# - Reseat card in PCIe slot
# - Verify both 8-pin power connectors attached
# - Check BIOS PCIe settings
```

### Issue: "CUDA illegal instruction" or "no kernel image"

**Cause**: PyTorch/libraries compiled for newer GPU architecture

**Solution**:
Our Docker image uses PyTorch 1.13.1 with Kepler support. If you see this error:

```bash
# Verify correct PyTorch version
docker run --rm hassistant/vision-worker:latest python3 -c "import torch; print(torch.__version__)"
# Should show: 1.13.1+cu117

# Rebuild image
docker build --no-cache -t hassistant/vision-worker:latest ./services/vision-worker/
```

### Issue: "GPU temperature > 80°C"

**Cause**: Insufficient cooling

**Solution**:
```bash
# Check current temps
nvidia-smi --query-gpu=temperature.gpu --format=csv

# Improve cooling:
# 1. Add/upgrade case fans
# 2. Ensure fans blow along K80 heatsink (front to back)
# 3. Increase fan speed
# 4. Lower ambient temperature
# 5. Check for dust buildup

# Reduce power limit temporarily
sudo nvidia-smi -i 2 -pl 200
sudo nvidia-smi -i 3 -pl 200
```

### Issue: "OpenCV built without CUDA support"

**Cause**: OpenCV-Python wheels lack CUDA support

**Solution**:
This is expected. OpenCV CUDA operations will fall back to CPU:
```
⚠️  OpenCV built without CUDA support. Using CPU fallback.
```

For full OpenCV CUDA support, you need to build from source:
```dockerfile
# In Dockerfile, replace:
# pip install opencv-contrib-python-headless
# With custom OpenCV build (adds ~30min to build time)
```

Our default configuration uses CPU for OpenCV ops and GPU for PyTorch/YOLO, which still provides good performance.

### Issue: "Only one K80 GPU visible"

**Cause**: K80 board not fully initialized or BIOS settings

**Solution**:
```bash
# Check PCIe topology
nvidia-smi topo -m

# Check BIOS:
# - Enable "Above 4G Decoding"
# - Enable "Resizable BAR" (if supported)
# - Set PCIe to Gen3 x16

# Cold reboot (not just restart)
sudo shutdown -h now
# Wait 30 seconds, then power on
```

### Issue: "Worker fails to start"

**Cause**: Port conflict or missing dependencies

**Solution**:
```bash
# Check logs
docker logs vision-worker-screen

# Check port availability
sudo netstat -tlnp | grep 8089

# Rebuild image
make k80-build

# Try manual run for debugging
docker run --rm -it --gpus device=2 \
  -e VISION_CUDA_DEVICE=2 \
  -e VISION_ROLE=screen \
  hassistant/vision-worker:latest \
  python3 -m app.main
```

### Issue: "Out of memory" (OOM)

**Cause**: Model too large for 12GB VRAM

**Solution**:
```bash
# Use smaller detector model
# Edit app/main.py:
# detector_model = YOLO("yolov8n.pt")  # 3MB model, ~500MB VRAM
# Instead of:
# detector_model = YOLO("yolov8x.pt")  # Larger model

# Check VRAM usage
nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

### Issue: "Driver version mismatch"

**Cause**: Host driver doesn't match container CUDA version

**Solution**:
```bash
# Check versions
nvidia-smi  # Host driver version
docker run --rm nvidia/cuda:11.4.3-base nvidia-smi  # Container test

# K80 requires driver >= 470.x for CUDA 11.4
# Upgrade if needed:
sudo apt install nvidia-driver-515
sudo reboot
```

## Known Limitations

### No Tensor Cores
K80 (Kepler) lacks Tensor Cores found in newer GPUs. This means:
- No FP16/INT8 acceleration
- Slower inference vs. newer cards (but still faster than CPU)
- Avoid libraries requiring Tensor Core operations

### CUDA 11.4 Maximum
K80 support ends at CUDA 11.4:
- PyTorch 1.13.x (last version with full Kepler support)
- TensorFlow 2.11.x
- Modern PyTorch 2.x may work but without optimizations

### No Display Output
K80 is compute-only:
- Cannot connect monitors
- No X11/Wayland support
- No video encoding (NVENC)

### Power & Cooling
Each K80 board draws 300W:
- Ensure adequate PSU capacity
- Requires active cooling (2x 120mm fans minimum)
- May be loud under load

## Performance Expectations

### Typical Workloads

| Task | FPS (K80) | FPS (CPU) | Speedup |
|------|-----------|-----------|---------|
| OpenCV CUDA resize/color | 40-50 | 15-20 | 2.5-3x |
| YOLOv8n (640x640) | 25-30 | 5-8 | 4-5x |
| YOLOv5n (640x640) | 30-35 | 6-10 | 4-5x |
| OCR (PaddleOCR CPU) | N/A | 8-10 imgs/s | N/A |

### Comparison to Other GPUs

| GPU | VRAM | YOLO FPS | Power | Notes |
|-----|------|----------|-------|-------|
| Tesla K80 | 12GB | 25-30 | 150W | Great value, good for batch |
| GTX 1070 | 8GB | 60-80 | 150W | Faster, less VRAM |
| GTX 1080 Ti | 11GB | 90-120 | 250W | 3-4x faster than K80 |
| RTX 3060 | 12GB | 150-200 | 170W | Tensor Cores, 6x faster |

**Recommendation**: Use K80 for batch preprocessing and lighter models. Keep heavy models (Qwen-VL) on modern GPUs (1080 Ti, RTX series).

## Integration Examples

### With vision-gateway

```python
# In vision-gateway, offload preprocessing to K80 workers
import requests

# Send frame to K80 worker
response = requests.post(
    "http://vision-worker-screen:8089/process/frame",
    files={"file": frame_bytes}
)
detections = response.json()["detections"]
```

### Docker Compose Integration

```yaml
services:
  my-service:
    environment:
      - VISION_WORKER_SCREEN_URL=http://vision-worker-screen:8089
      - VISION_WORKER_ROOM_URL=http://vision-worker-room:8090
    depends_on:
      - vision-screen
      - vision-room
```

## Maintenance

### Regular Tasks

**Daily**:
- Check temperatures: `make k80-stats`
- Review logs for errors: `make k80-logs`

**Weekly**:
- Run health check: `make k80-warmup`
- Verify GPU utilization is appropriate

**Monthly**:
- Run burn-in test: `make k80-burnin`
- Clean dust from fans/heatsink
- Check driver updates: `ubuntu-drivers devices`

### Upgrading

To upgrade vision-worker:
```bash
# Pull latest code
git pull

# Rebuild image
make k80-build

# Restart workers
make k80-down
make k80-up

# Verify health
make k80-health
```

## FAQ

**Q: Can I use K80 for LLM inference?**
A: Yes, but limited by CUDA 11.4 max. Modern LLM frameworks prefer newer GPUs. Best for smaller models or quantized versions.

**Q: What if I only have one K80 GPU?**
A: Assign both workers to the same GPU:
```bash
VISION_SCREEN_CUDA_DEVICE=2
VISION_ROOM_CUDA_DEVICE=2
```

**Q: Can I mix K80 with other GPUs?**
A: Yes! Our setup uses GTX 1080 Ti (GPU0), GTX 1070 (GPU1), and K80s (GPU2-3).

**Q: Does K80 work with Windows?**
A: Yes, but cooling is more challenging. Linux recommended for server use.

**Q: What about K80 vs K40?**
A: K40 has single GPU (12GB), K80 has dual GPU (2x12GB). K80 is better value.

## Resources

- [NVIDIA Tesla K80 Datasheet](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/Tesla-K80-BoardSpec-07317-001-v05.pdf)
- [Kepler Architecture Whitepaper](https://www.nvidia.com/content/PDF/kepler/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf)
- [CUDA Compatibility](https://docs.nvidia.com/deploy/cuda-compatibility/)
- [PyTorch CUDA Wheels](https://download.pytorch.org/whl/torch_stable.html)

## Support

For issues or questions:
1. Check this troubleshooting guide
2. Review logs: `make k80-logs`
3. Run diagnostics: `make k80-warmup`
4. Open GitHub issue with logs and `nvidia-smi` output
