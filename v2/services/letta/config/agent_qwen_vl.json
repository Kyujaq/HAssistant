{
  "id": "agent_qwen_vl",
  "name": "Qwen-VL Specialist",
  "description": "Vision and deep reasoning participant that handles complex or multi-modal requests.",
  "llm_config": {
    "model": "qwen2.5vl:7b",
    "model_endpoint_type": "ollama",
    "model_endpoint": "http://ollama-vl:11434/v1",
    "context_window": 4096,
    "max_tokens": 1024,
    "temperature": 0.2,
    "put_inner_thoughts_in_kwargs": true
  },
  "embedding_config": {
    "embedding_endpoint_type": "ollama",
    "embedding_endpoint": "http://ollama-text:11434/v1",
    "embedding_model": "nomic-embed-text",
    "embedding_dim": 768,
    "embedding_chunk_size": 300,
    "batch_size": 32
  },
  "instructions": "You are the heavy vision-language specialist. When images or complex reasoning are required, analyse inputs, summarise findings, and recommend actions back to the orchestrator. Do not call ha_call_service directly. Report any auto-downgraded model via used_model metadata.",
  "tools": [
    "send_message",
    "vision_analyze",
    "memory_write",
    "memory_update",
    "memory_search"
  ],
  "memory_blocks": [
    "persona",
    "human",
    "identity_preferences",
    "home_graph",
    "system_facts",
    "routines_playbooks"
  ],
  "config": {
    "max_completion_tokens": 2048,
    "timeout_seconds": 6,
    "temperature": 0.2,
    "stream_steps": false
  },
  "metadata": {
    "phase": "3",
    "role": "participant"
  }
}
