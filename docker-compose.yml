services:
  # Disabled old ollama service - using ollama-chat and ollama-vision instead
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: hassistant-ollama
  #   runtime: nvidia
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['0', '1']  # Both GTX 1080 Ti and GTX 1070
  #             capabilities: [gpu]
  #   environment:
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - OLLAMA_HOST=0.0.0.0:11434
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ./ollama:/root/.ollama  # Persistent model storage
  #     - /home/qjaq/assistant/models:/models:ro  # Read-only access to existing models
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 10s
  #   networks:
  #     - ha_network

  # Wyoming Whisper STT (Speech-to-Text) - GPU Accelerated
  whisper:
    image: rhasspy/wyoming-whisper:latest
    container_name: hassistant-whisper
    runtime: nvidia
    command: --model small --language en --uri 'tcp://0.0.0.0:10300' --data-dir /data --device cuda
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['1']  # GPU 1 → GTX 1070 (lighter model, good for STT)
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
    volumes:
      - ./whisper_data:/data
    ports:
      - "10300:10300"
    restart: unless-stopped
    networks:
      - ha_network

  # Wyoming Piper TTS (Text-to-Speech) - GLaDOS voice - GPU Accelerated
  piper-glados:
    image: rhasspy/wyoming-piper:latest
    container_name: hassistant-piper-glados
    runtime: nvidia
    command: --voice en_US-glados-medium --uri 'tcp://0.0.0.0:10200' --data-dir /data --piper /usr/share/piper/piper
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['1']  # GPU 1 → GTX 1070 (share with Whisper)
    environment:
      - NVIDIA_VISIBLE_DEVICES=1
    volumes:
      - ./piper_data:/data
    ports:
      - "10200:10200"
    restart: unless-stopped
    networks:
      - ha_network

  # Home Assistant
  homeassistant:
    image: ghcr.io/home-assistant/home-assistant:stable
    container_name: hassistant-homeassistant
    volumes:
      - /home/qjaq/assistant/data/ha_config:/config
      - /etc/localtime:/etc/localtime:ro
    ports:
      - "8123:8123"
    restart: unless-stopped
    privileged: true
    networks:
      - ha_network

  # PostgreSQL with pgvector for Letta memory
  postgres:
    image: pgvector/pgvector:pg15
    container_name: hassistant-postgres
    environment:
      - POSTGRES_DB=hassistant
      - POSTGRES_USER=hassistant
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-hassistant_secure_password}
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts:/docker-entrypoint-initdb.d:ro
    ports:
      - "5433:5432"  # Use 5433 to avoid conflict with /assistant postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hassistant -d hassistant"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - ha_network

  # Redis for session cache and ephemeral data
  redis:
    image: redis:7-alpine
    container_name: hassistant-redis
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-hassistant_redis_password}
    volumes:
      - redis_data:/data
    ports:
      - "6380:6379"  # Use 6380 to avoid conflict with /assistant redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD:-hassistant_redis_password}", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    networks:
      - ha_network

  # Letta Bridge API
  letta-bridge:
    build: ./letta_bridge
    container_name: hassistant-letta-bridge
    env_file: .env
    restart: unless-stopped
    ports:
      - "8081:8081"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "-H", "x-api-key: ${BRIDGE_API_KEY:-change-me-to-secure-key}", "http://localhost:8081/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ha_network

  # Qwen-Agent for AI orchestration
  qwen-agent:
    build:
      context: ./qwen-agent
      dockerfile: Dockerfile
    container_name: hassistant-qwen-agent
    env_file: .env
    depends_on:
      - letta-bridge
    restart: unless-stopped
    networks:
      - ha_network
    # Optional: mount your data/tools/configs
    volumes:
      - ./agent_data:/data

  # GLaDOS Orchestrator - Routes queries between Qwen (brain) and Hermes (personality)
  glados-orchestrator:
    build:
      context: ./glados-orchestrator
      dockerfile: Dockerfile
    container_name: hassistant-glados-orchestrator
    environment:
      - OLLAMA_BASE_URL=http://ollama-chat:11434
      - QWEN_MODEL=qwen2.5:7b
      - HERMES_MODEL=glados-hermes3
      - PORT=8082
    ports:
      - "8082:8082"
    depends_on:
      - ollama-chat
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8082/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ha_network

  # ---- Vision Gateway (CPU heavy, light GPU use unless you enable GPU OCR) ----
  vision-gateway:
    build:
      context: ./vision-gateway
      dockerfile: Dockerfile
    container_name: vision-gateway
    environment:
      # HA push
      - HA_BASE_URL=http://homeassistant:8123
      - HA_TOKEN=${HA_TOKEN}
      # VL endpoint (1070 / Ollama-vision)
      - OLLAMA_VISION_BASE=http://ollama-vision:11434
      - OLLAMA_VISION_MODEL=qwen2.5vl:7b
      # Optional: enable internal HDMI pull
      - HDMI_DEVICE=/dev/video0
      - HDMI_ENABLED=true
      - HDMI_FPS=2
      - HDMI_RESIZE_LONG=1280
      # Motion & debounce
      - MOTION_THRESHOLD=0.015
      - COOLDOWN_SECONDS=8
    volumes:
      - ./vision-gateway/cache:/app/cache
    devices:
      - "/dev/video0:/dev/video0"    # your HDMI dongle
    depends_on:
      - homeassistant
    restart: unless-stopped
    networks:
      - ha_network

  # ---- Frigate (webcam motion, snapshots) ----
  frigate:
    image: ghcr.io/blakeblackshear/frigate:stable
    container_name: frigate
    privileged: true
    shm_size: "512m"
    environment:
      - FRIGATE_RTSP_PASSWORD=${FRIGATE_RTSP_PASSWORD:-password}
    volumes:
      - ./frigate/config:/config
      - /etc/localtime:/etc/localtime:ro
      - ./frigate/media:/media/frigate
    ports:
      - "5000:5000"     # Frigate UI/API
    restart: unless-stopped
    networks:
      - ha_network

  # Optional helper to snapshot webcam via Frigate API (no GPU)
  frigate-snapshotper:
    image: curlimages/curl:8.10.1
    container_name: frigate-snapshotper
    command: "sleep infinity"
    networks:
      - ha_network

  # Your ollama containers should already exist:
  # - ollama-chat on GPU0 (11434) with Qwen3-* for chat
  # - ollama-vision on GPU1 (11435->11434 in container) with qwen2.5vl:7b
  # Ensure they're reachable as ollama-chat / ollama-vision or use host IPs.
  
  ollama-chat:
    image: ollama/ollama:latest
    container_name: ollama-chat
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['0']   # GPU 0 → 1080 Ti
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - ollama_chat_data:/root/.ollama
      - ./ollama/modelfiles:/root/.ollama/modelfiles:ro
      - /home/qjaq/assistant/models:/models:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ha_network

  ollama-vision:
    image: ollama/ollama:latest
    container_name: ollama-vision
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ['1']   # GPU 1 → 1070
    ports:
      - "11435:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - NVIDIA_VISIBLE_DEVICES=1
    volumes:
      - ollama_vision_data:/root/.ollama
      - ./ollama/modelfiles:/root/.ollama/modelfiles:ro
      - /home/qjaq/assistant/models:/models:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - ha_network
  
  
  

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  ollama_chat_data:
    driver: local
  ollama_vision_data:
    driver: local

networks:
  ha_network:
    external: true
    name: assistant_default  # Connect to existing HA network
